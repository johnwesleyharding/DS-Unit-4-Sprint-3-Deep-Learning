{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "# r = requests.get(url)\n",
    "# r.encoding = r.apparent_encoding\n",
    "# data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    txt_data.splitlines()\n",
    "    txt_data.rstrip('\\n') # didnt work, but maybe it's better with them\n",
    "    txt_data.split('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_data = open('sonnets.txt', 'r').read() # test external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = list(set(txt_data))\n",
    "# num_chars = len(chars)\n",
    "# txt_data_size = len(txt_data)\n",
    "# print(f'Total characters: {txt_data_size}. Unique characters: {num_chars}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# integer_encoded = [char_to_int[i] for i in txt_data] # could be used for manual one-hot encode\n",
    "# print(int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually copied the block of 154 sonnets into notepad, then used find and replace to make the fancy apostrophes a normal character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sonnets.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    1\n",
      "\n",
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world's fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And, tender churl, mak'st waste in niggarding:\n",
      "  Pity the world, or else this glutton be,\n",
      "  To eat the world's due, by the grave and thee.\n",
      "                    2\n",
      "\n",
      "When forty winters shall besiege thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery so gazed on now,\n",
      "Will be a tattered weed of small worth held:\n",
      "Then being asked, where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days;\n",
      "To say, within thine own deep sunken eyes,\n",
      "Were an all-eating shame, and thriftless praise.\n",
      "How much more praise deserv'd thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count, and make my old excuse,'\n",
      "Proving his beauty by succession thine.\n",
      "  This were to be new made when thou art old,\n",
      "  And see thy blood warm when thou feel'st it cold.\n",
      "                    3\n",
      "\n",
      "Look in thy glass and tell the face thou viewest,\n",
      "Now is the time that face should form another,\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose uneared womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb\n",
      "Of his self-love to stop posterity?\n",
      "Thou art thy mother's glass and she in thee\n",
      "Calls back the lovely April of her prime,\n",
      "So thou through windows of thine age shalt see,\n",
      "Despite of wrinkles this thy golden time.\n",
      "  But if thou live remembered not to be,\n",
      "  Die single and thine image dies with thee.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(text.split('\\n\\n\\n')[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [char_int[c] for c in text]\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 6114\n"
     ]
    }
   ],
   "source": [
    "maxlen = 512\n",
    "step = 16\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6114, 512, 71), (6114, 71))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0128 01:50:29.545747 14712 deprecation.py:506] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    if epoch % 32 == 0:\n",
    "    \n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        generated = ''\n",
    "\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(128):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature=1.0)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0128 01:50:29.888653 14712 deprecation.py:323] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "5120/6114 [========================>.....] - ETA: 10s - loss: 4.2275\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ft you, mine eye is in my mind,\n",
      "And that which governs me to go about,\n",
      "Doth part his function, and is partly blind,\n",
      "Seems seeing, but effectually is out:\n",
      "For it no form delivers to the heart\n",
      "Of bird, of flower, or shape which it doth latch,\n",
      "Of his quick objects hath the mind no part,\n",
      "Nor his own vision holds what it doth catch:\n",
      "For if it see the rud'st or gentlest sight,\n",
      "The most sweet favour or deformed'st creature,\n",
      "The mountain, or the sea, the day, or night:\n",
      "The crow, or dove, it shapes them to your feat\"\n",
      "ft you, mine eye is in my mind,\n",
      "And that which governs me to go about,\n",
      "Doth part his function, and is partly blind,\n",
      "Seems seeing, but effectually is out:\n",
      "For it no form delivers to the heart\n",
      "Of bird, of flower, or shape which it doth latch,\n",
      "Of his quick objects hath the mind no part,\n",
      "Nor his own vision holds what it doth catch:\n",
      "For if it see the rud'st or gentlest sight,\n",
      "The most sweet favour or deformed'st creature,\n",
      "The mountain, or the sea, the day, or night:\n",
      "The crow, or dove, it shapes them to your featcr9Bw;;\n",
      "9PTlz8wixA,eA.W3(ffO::ehNP)pxNsa05qwPh6WEFtVjA2,cGy7(g-gI4iKB8P9h2r6xRuEgbrBc0WTYLsgShsrT3J.s.fsuFh:Ad1nsz2W-aVatl Ccij)\n",
      "6114/6114 [==============================] - 78s 13ms/sample - loss: 4.2098\n",
      "Epoch 2/256\n",
      "6114/6114 [==============================] - 65s 11ms/sample - loss: 3.4424\n",
      "Epoch 3/256\n",
      "6114/6114 [==============================] - 65s 11ms/sample - loss: 3.1382\n",
      "Epoch 4/256\n",
      "6114/6114 [==============================] - 66s 11ms/sample - loss: 3.0858\n",
      "Epoch 5/256\n",
      "6114/6114 [==============================] - 69s 11ms/sample - loss: 3.0625\n",
      "Epoch 6/256\n",
      "6114/6114 [==============================] - 67s 11ms/sample - loss: 3.0574\n",
      "Epoch 7/256\n",
      "6114/6114 [==============================] - 69s 11ms/sample - loss: 3.0452\n",
      "Epoch 8/256\n",
      "6114/6114 [==============================] - 70s 11ms/sample - loss: 3.0397\n",
      "Epoch 9/256\n",
      "6114/6114 [==============================] - 71s 12ms/sample - loss: 3.0321\n",
      "Epoch 10/256\n",
      "6114/6114 [==============================] - 71s 12ms/sample - loss: 3.0287\n",
      "Epoch 11/256\n",
      "6114/6114 [==============================] - 72s 12ms/sample - loss: 3.0215\n",
      "Epoch 12/256\n",
      "6114/6114 [==============================] - 72s 12ms/sample - loss: 3.0163\n",
      "Epoch 13/256\n",
      "6114/6114 [==============================] - 73s 12ms/sample - loss: 3.0109\n",
      "Epoch 14/256\n",
      "6114/6114 [==============================] - 74s 12ms/sample - loss: 3.0048\n",
      "Epoch 15/256\n",
      "6114/6114 [==============================] - 74s 12ms/sample - loss: 2.9974\n",
      "Epoch 16/256\n",
      "6114/6114 [==============================] - 75s 12ms/sample - loss: 2.9894\n",
      "Epoch 17/256\n",
      "6114/6114 [==============================] - 76s 12ms/sample - loss: 2.9812\n",
      "Epoch 18/256\n",
      "6114/6114 [==============================] - 77s 13ms/sample - loss: 2.9726\n",
      "Epoch 19/256\n",
      "6114/6114 [==============================] - 77s 13ms/sample - loss: 2.9598\n",
      "Epoch 20/256\n",
      "6114/6114 [==============================] - 78s 13ms/sample - loss: 2.9482\n",
      "Epoch 21/256\n",
      "6114/6114 [==============================] - 79s 13ms/sample - loss: 2.9316\n",
      "Epoch 22/256\n",
      "6114/6114 [==============================] - 78s 13ms/sample - loss: 2.9162\n",
      "Epoch 23/256\n",
      "6114/6114 [==============================] - 79s 13ms/sample - loss: 2.9006\n",
      "Epoch 24/256\n",
      "6114/6114 [==============================] - 79s 13ms/sample - loss: 2.8826\n",
      "Epoch 25/256\n",
      "6114/6114 [==============================] - 80s 13ms/sample - loss: 2.8565\n",
      "Epoch 26/256\n",
      "6114/6114 [==============================] - 80s 13ms/sample - loss: 2.8332\n",
      "Epoch 27/256\n",
      "6114/6114 [==============================] - 81s 13ms/sample - loss: 2.8055\n",
      "Epoch 28/256\n",
      "6114/6114 [==============================] - 81s 13ms/sample - loss: 2.7746\n",
      "Epoch 29/256\n",
      "6114/6114 [==============================] - 81s 13ms/sample - loss: 2.7458\n",
      "Epoch 30/256\n",
      "6114/6114 [==============================] - 81s 13ms/sample - loss: 2.7124\n",
      "Epoch 31/256\n",
      "6114/6114 [==============================] - 83s 13ms/sample - loss: 2.6762\n",
      "Epoch 32/256\n",
      "6114/6114 [==============================] - 83s 14ms/sample - loss: 2.6376\n",
      "Epoch 33/256\n",
      "5120/6114 [========================>.....] - ETA: 12s - loss: 2.5935\n",
      "----- Generating text after Epoch: 32\n",
      "----- Generating with seed: \"\n",
      "\n",
      "Then hate me when thou wilt, if ever, now,\n",
      "Now while the world is bent my deeds to cross,\n",
      "join with the spite of fortune, make me bow,\n",
      "And do not drop in for an after-loss:\n",
      "Ah do not, when my heart hath 'scaped this sorrow,\n",
      "Come in the rearward of a conquered woe,\n",
      "Give not a windy night a rainy morrow,\n",
      "To linger out a purposed overthrow.\n",
      "If thou wilt leave me, do not leave me last,\n",
      "When other petty griefs have done their spite,\n",
      "But in the onset come, so shall I taste\n",
      "At first the very worst of fortune's m\"\n",
      "\n",
      "\n",
      "Then hate me when thou wilt, if ever, now,\n",
      "Now while the world is bent my deeds to cross,\n",
      "join with the spite of fortune, make me bow,\n",
      "And do not drop in for an after-loss:\n",
      "Ah do not, when my heart hath 'scaped this sorrow,\n",
      "Come in the rearward of a conquered woe,\n",
      "Give not a windy night a rainy morrow,\n",
      "To linger out a purposed overthrow.\n",
      "If thou wilt leave me, do not leave me last,\n",
      "When other petty griefs have done their spite,\n",
      "But in the onset come, so shall I taste\n",
      "At first the very worst of fortune's mod h dh\n",
      " ih:,\n",
      "oan hecaghls T mes tnhe sov iws,r udere  pn  xf ke,i tn  tillsts,segdog es nf ershrllgtthe nd Sore dxrre dWybthr s\n",
      "6114/6114 [==============================] - 101s 16ms/sample - loss: 2.6017\n",
      "Epoch 34/256\n",
      "6114/6114 [==============================] - 83s 14ms/sample - loss: 2.5718\n",
      "Epoch 35/256\n",
      "6114/6114 [==============================] - 84s 14ms/sample - loss: 2.5349\n",
      "Epoch 36/256\n",
      "6114/6114 [==============================] - 86s 14ms/sample - loss: 2.5022\n",
      "Epoch 37/256\n",
      "6114/6114 [==============================] - 88s 14ms/sample - loss: 2.4871\n",
      "Epoch 38/256\n",
      "6114/6114 [==============================] - 87s 14ms/sample - loss: 2.4536\n",
      "Epoch 39/256\n",
      "6114/6114 [==============================] - 86s 14ms/sample - loss: 2.4316\n",
      "Epoch 40/256\n",
      "6114/6114 [==============================] - 87s 14ms/sample - loss: 2.4007\n",
      "Epoch 41/256\n",
      "6114/6114 [==============================] - 89s 15ms/sample - loss: 2.3757\n",
      "Epoch 42/256\n",
      "6114/6114 [==============================] - 92s 15ms/sample - loss: 2.3493\n",
      "Epoch 43/256\n",
      "6114/6114 [==============================] - 87s 14ms/sample - loss: 2.3341\n",
      "Epoch 44/256\n",
      "6114/6114 [==============================] - 89s 15ms/sample - loss: 2.3088\n",
      "Epoch 45/256\n",
      "6114/6114 [==============================] - 88s 14ms/sample - loss: 2.2954\n",
      "Epoch 46/256\n",
      "6114/6114 [==============================] - 88s 14ms/sample - loss: 2.2894\n",
      "Epoch 47/256\n",
      "6114/6114 [==============================] - 87s 14ms/sample - loss: 2.2707\n",
      "Epoch 48/256\n",
      "6114/6114 [==============================] - 88s 14ms/sample - loss: 2.2550\n",
      "Epoch 49/256\n",
      "6114/6114 [==============================] - 94s 15ms/sample - loss: 2.2437\n",
      "Epoch 50/256\n",
      "6114/6114 [==============================] - 92s 15ms/sample - loss: 2.2217\n",
      "Epoch 51/256\n",
      "6114/6114 [==============================] - 91s 15ms/sample - loss: 2.2110\n",
      "Epoch 52/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.1919\n",
      "Epoch 53/256\n",
      "6114/6114 [==============================] - 96s 16ms/sample - loss: 2.1811\n",
      "Epoch 54/256\n",
      "6114/6114 [==============================] - 99s 16ms/sample - loss: 2.1634\n",
      "Epoch 55/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.1564\n",
      "Epoch 56/256\n",
      "6114/6114 [==============================] - 98s 16ms/sample - loss: 2.1524\n",
      "Epoch 57/256\n",
      "6114/6114 [==============================] - 99s 16ms/sample - loss: 2.1322\n",
      "Epoch 58/256\n",
      "6114/6114 [==============================] - 98s 16ms/sample - loss: 2.1279\n",
      "Epoch 59/256\n",
      "6114/6114 [==============================] - 99s 16ms/sample - loss: 2.1192\n",
      "Epoch 60/256\n",
      "6114/6114 [==============================] - 99s 16ms/sample - loss: 2.1112\n",
      "Epoch 61/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.0937\n",
      "Epoch 62/256\n",
      "6114/6114 [==============================] - 100s 16ms/sample - loss: 2.0777\n",
      "Epoch 63/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.0728\n",
      "Epoch 64/256\n",
      "6114/6114 [==============================] - 98s 16ms/sample - loss: 2.0543\n",
      "Epoch 65/256\n",
      "5120/6114 [========================>.....] - ETA: 14s - loss: 2.0539\n",
      "----- Generating text after Epoch: 64\n",
      "----- Generating with seed: \"even.\n",
      "  But day doth daily draw my sorrows longer,\n",
      "  And night doth nightly make grief's length seem stronger\n",
      "\n",
      "\n",
      "                    29\n",
      "\n",
      "When in disgrace with Fortune and men's eyes,\n",
      "I all alone beweep my outcast state,\n",
      "And trouble deaf heaven with my bootless cries,\n",
      "And look upon my self and curse my fate,\n",
      "Wishing me like to one more rich in hope,\n",
      "Featured like him, like him with friends possessed,\n",
      "Desiring this man's art, and that man's scope,\n",
      "With what I most enjoy contented least,\n",
      "Yet in these thoughts m\"\n",
      "even.\n",
      "  But day doth daily draw my sorrows longer,\n",
      "  And night doth nightly make grief's length seem stronger\n",
      "\n",
      "\n",
      "                    29\n",
      "\n",
      "When in disgrace with Fortune and men's eyes,\n",
      "I all alone beweep my outcast state,\n",
      "And trouble deaf heaven with my bootless cries,\n",
      "And look upon my self and curse my fate,\n",
      "Wishing me like to one more rich in hope,\n",
      "Featured like him, like him with friends possessed,\n",
      "Desiring this man's art, and that man's scope,\n",
      "With what I most enjoy contented least,\n",
      "Yet in these thoughts melt,\n",
      "hare hee veate ceimelsdertmen the gfomes men angossfenchere.\n",
      " orcm my ove ,eistyoangee hry ur wfrthany moek chaef het ullse\n",
      "6114/6114 [==============================] - 120s 20ms/sample - loss: 2.0473\n",
      "Epoch 66/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.0402\n",
      "Epoch 67/256\n",
      "6114/6114 [==============================] - 97s 16ms/sample - loss: 2.0315\n",
      "Epoch 68/256\n",
      "6114/6114 [==============================] - 98s 16ms/sample - loss: 2.0222\n",
      "Epoch 69/256\n",
      "6114/6114 [==============================] - 100s 16ms/sample - loss: 2.0025\n",
      "Epoch 70/256\n",
      "6114/6114 [==============================] - 103s 17ms/sample - loss: 1.9925\n",
      "Epoch 71/256\n",
      "6114/6114 [==============================] - 101s 17ms/sample - loss: 1.9847\n",
      "Epoch 72/256\n",
      "6114/6114 [==============================] - 102s 17ms/sample - loss: 1.9712\n",
      "Epoch 73/256\n",
      "6114/6114 [==============================] - 101s 16ms/sample - loss: 1.9609\n",
      "Epoch 74/256\n",
      "6114/6114 [==============================] - 103s 17ms/sample - loss: 1.9536\n",
      "Epoch 75/256\n",
      "6114/6114 [==============================] - 103s 17ms/sample - loss: 1.9498\n",
      "Epoch 76/256\n",
      "6114/6114 [==============================] - 102s 17ms/sample - loss: 1.9374\n",
      "Epoch 77/256\n",
      "6114/6114 [==============================] - 100s 16ms/sample - loss: 1.9303\n",
      "Epoch 78/256\n",
      "6114/6114 [==============================] - 103s 17ms/sample - loss: 1.9183\n",
      "Epoch 79/256\n",
      "6114/6114 [==============================] - 100s 16ms/sample - loss: 1.9099\n",
      "Epoch 80/256\n",
      "6114/6114 [==============================] - 104s 17ms/sample - loss: 1.8990\n",
      "Epoch 81/256\n",
      "6114/6114 [==============================] - 104s 17ms/sample - loss: 1.8919\n",
      "Epoch 82/256\n",
      "6114/6114 [==============================] - 101s 17ms/sample - loss: 1.8725\n",
      "Epoch 83/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.8610\n",
      "Epoch 84/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.8632\n",
      "Epoch 85/256\n",
      "6114/6114 [==============================] - 105s 17ms/sample - loss: 1.8499\n",
      "Epoch 86/256\n",
      "6114/6114 [==============================] - 103s 17ms/sample - loss: 1.8426\n",
      "Epoch 87/256\n",
      "6114/6114 [==============================] - 104s 17ms/sample - loss: 1.8320\n",
      "Epoch 88/256\n",
      "6114/6114 [==============================] - 105s 17ms/sample - loss: 1.8230\n",
      "Epoch 89/256\n",
      "6114/6114 [==============================] - 105s 17ms/sample - loss: 1.8023\n",
      "Epoch 90/256\n",
      "6114/6114 [==============================] - 100s 16ms/sample - loss: 1.7956\n",
      "Epoch 91/256\n",
      "6114/6114 [==============================] - 104s 17ms/sample - loss: 1.7798\n",
      "Epoch 92/256\n",
      "6114/6114 [==============================] - 105s 17ms/sample - loss: 1.7705\n",
      "Epoch 93/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.7555\n",
      "Epoch 94/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.7399\n",
      "Epoch 95/256\n",
      "6114/6114 [==============================] - 107s 18ms/sample - loss: 1.7268\n",
      "Epoch 96/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.7142\n",
      "Epoch 97/256\n",
      "5120/6114 [========================>.....] - ETA: 15s - loss: 1.7045\n",
      "----- Generating text after Epoch: 96\n",
      "----- Generating with seed: \"ls the time,\n",
      "And see the brave day sunk in hideous night,\n",
      "When I behold the violet past prime,\n",
      "And sable curls all silvered o'er with white:\n",
      "When lofty trees I see barren of leaves,\n",
      "Which erst from heat did canopy the herd\n",
      "And summer's green all girded up in sheaves\n",
      "Borne on the bier with white and bristly beard:\n",
      "Then of thy beauty do I question make\n",
      "That thou among the wastes of time must go,\n",
      "Since sweets and beauties do themselves forsake,\n",
      "And die as fast as they see others grow,\n",
      "  And nothing 'gainst Tim\"\n",
      "ls the time,\n",
      "And see the brave day sunk in hideous night,\n",
      "When I behold the violet past prime,\n",
      "And sable curls all silvered o'er with white:\n",
      "When lofty trees I see barren of leaves,\n",
      "Which erst from heat did canopy the herd\n",
      "And summer's green all girded up in sheaves\n",
      "Borne on the bier with white and bristly beard:\n",
      "Then of thy beauty do I question make\n",
      "That thou among the wastes of time must go,\n",
      "Since sweets and beauties do themselves forsake,\n",
      "And die as fast as they see others grow,\n",
      "  And nothing 'gainst Timy inawsin ghiderond oyosereshe,\n",
      "Hre hamendsee se sn ealfarif whit withnane delerss shyome,\n",
      "y cucl whay om tits ake gael elt or,\n",
      "\n",
      "6114/6114 [==============================] - 132s 22ms/sample - loss: 1.7189\n",
      "Epoch 98/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.7137\n",
      "Epoch 99/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.7054\n",
      "Epoch 100/256\n",
      "6114/6114 [==============================] - 107s 17ms/sample - loss: 1.6800\n",
      "Epoch 101/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.6744\n",
      "Epoch 102/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.6623\n",
      "Epoch 103/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.6374\n",
      "Epoch 104/256\n",
      "6114/6114 [==============================] - 109s 18ms/sample - loss: 1.6212\n",
      "Epoch 105/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.6023\n",
      "Epoch 106/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.5897\n",
      "Epoch 107/256\n",
      "6114/6114 [==============================] - 107s 17ms/sample - loss: 1.5652\n",
      "Epoch 108/256\n",
      "6114/6114 [==============================] - 106s 17ms/sample - loss: 1.5637\n",
      "Epoch 109/256\n",
      "6114/6114 [==============================] - 107s 18ms/sample - loss: 1.5531\n",
      "Epoch 110/256\n",
      "6114/6114 [==============================] - 108s 18ms/sample - loss: 1.5417\n",
      "Epoch 111/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 1.5037\n",
      "Epoch 112/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 1.4887\n",
      "Epoch 113/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.4664\n",
      "Epoch 114/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.4692\n",
      "Epoch 115/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.4752\n",
      "Epoch 116/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.4553\n",
      "Epoch 117/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.4321\n",
      "Epoch 118/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.4039\n",
      "Epoch 119/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.3742\n",
      "Epoch 120/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 1.3461\n",
      "Epoch 121/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.3234\n",
      "Epoch 122/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.3215\n",
      "Epoch 123/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.3122\n",
      "Epoch 124/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.3049\n",
      "Epoch 125/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.2819\n",
      "Epoch 126/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.2401\n",
      "Epoch 127/256\n",
      "6114/6114 [==============================] - 109s 18ms/sample - loss: 1.2571\n",
      "Epoch 128/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 1.2296\n",
      "Epoch 129/256\n",
      "5120/6114 [========================>.....] - ETA: 15s - loss: 1.2322\n",
      "----- Generating text after Epoch: 128\n",
      "----- Generating with seed: \"               36\n",
      "\n",
      "Let me confess that we two must be twain,\n",
      "Although our undivided loves are one:\n",
      "So shall those blots that do with me remain,\n",
      "Without thy help, by me be borne alone.\n",
      "In our two loves there is but one respect,\n",
      "Though in our lives a separable spite,\n",
      "Which though it alter not love's sole effect,\n",
      "Yet doth it steal sweet hours from love's delight.\n",
      "I may not evermore acknowledge thee,\n",
      "Lest my bewailed guilt should do thee shame,\n",
      "Nor thou with public kindness honour me,\n",
      "Unless thou take that hono\"\n",
      "               36\n",
      "\n",
      "Let me confess that we two must be twain,\n",
      "Although our undivided loves are one:\n",
      "So shall those blots that do with me remain,\n",
      "Without thy help, by me be borne alone.\n",
      "In our two loves there is but one respect,\n",
      "Though in our lives a separable spite,\n",
      "Which though it alter not love's sole effect,\n",
      "Yet doth it steal sweet hours from love's delight.\n",
      "I may not evermore acknowledge thee,\n",
      "Lest my bewailed guilt should do thee shame,\n",
      "Nor thou with public kindness honour me,\n",
      "Unless thou take that honosg, oo dgins coty ub..\n",
      "\n",
      "\n",
      "                    67\n",
      "\n",
      "An fhinge tu eay dids ov thit's'sing,\n",
      "Thart my barnas toul thi gerakr ing onsin\n",
      "6114/6114 [==============================] - 134s 22ms/sample - loss: 1.2298\n",
      "Epoch 130/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 1.2089\n",
      "Epoch 131/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.1791\n",
      "Epoch 132/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.1734\n",
      "Epoch 133/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.1467\n",
      "Epoch 134/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.1180\n",
      "Epoch 135/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.0902\n",
      "Epoch 136/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 1.0797\n",
      "Epoch 137/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.0549\n",
      "Epoch 138/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.0414\n",
      "Epoch 139/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 1.0130\n",
      "Epoch 140/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.0151\n",
      "Epoch 141/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 1.0116\n",
      "Epoch 142/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.9712\n",
      "Epoch 143/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.9477\n",
      "Epoch 144/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.9116\n",
      "Epoch 145/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.9122\n",
      "Epoch 146/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.9257\n",
      "Epoch 147/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.8791\n",
      "Epoch 148/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.8565\n",
      "Epoch 149/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.8281\n",
      "Epoch 150/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.8171\n",
      "Epoch 151/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.8196\n",
      "Epoch 152/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.8195\n",
      "Epoch 153/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.7796\n",
      "Epoch 154/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.7800\n",
      "Epoch 155/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.7662\n",
      "Epoch 156/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.7368\n",
      "Epoch 157/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.7385\n",
      "Epoch 158/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.7243\n",
      "Epoch 159/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.7027\n",
      "Epoch 160/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.6938\n",
      "Epoch 161/256\n",
      "5120/6114 [========================>.....] - ETA: 16s - loss: 0.6816\n",
      "----- Generating text after Epoch: 160\n",
      "----- Generating with seed: \"y love-suit sweet fulfil.\n",
      "Will will fulfil the treasure of thy love,\n",
      "Ay, fill it full with wills, and my will one,\n",
      "In things of great receipt with case we prove,\n",
      "Among a number one is reckoned none.\n",
      "Then in the number let me pass untold,\n",
      "Though in thy store's account I one must be,\n",
      "For nothing hold me, so it please thee hold,\n",
      "That nothing me, a something sweet to thee.\n",
      "  Make but my name thy love, and love that still,\n",
      "  And then thou lov'st me for my name is Will.\n",
      "\n",
      "\n",
      "                    137\n",
      "\n",
      "Thou blind fool \"\n",
      "y love-suit sweet fulfil.\n",
      "Will will fulfil the treasure of thy love,\n",
      "Ay, fill it full with wills, and my will one,\n",
      "In things of great receipt with case we prove,\n",
      "Among a number one is reckoned none.\n",
      "Then in the number let me pass untold,\n",
      "Though in thy store's account I one must be,\n",
      "For nothing hold me, so it please thee hold,\n",
      "That nothing me, a something sweet to thee.\n",
      "  Make but my name thy love, and love that still,\n",
      "  And then thou lov'st me for my name is Will.\n",
      "\n",
      "\n",
      "                    137\n",
      "\n",
      "Thou blind fool of as buutu ar ow the on fuy ay\n",
      "seat bended,\n",
      "When ain my bearty in ereor and.\n",
      "B  thoug thy calsssaaty howreco hurwert aptesen,\n",
      "A\n",
      "6114/6114 [==============================] - 138s 23ms/sample - loss: 0.6874\n",
      "Epoch 162/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.6622\n",
      "Epoch 163/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.6520\n",
      "Epoch 164/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.6496\n",
      "Epoch 165/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.6462\n",
      "Epoch 166/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.6326\n",
      "Epoch 167/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.6059\n",
      "Epoch 168/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.5772\n",
      "Epoch 169/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.5597\n",
      "Epoch 170/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.5474\n",
      "Epoch 171/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.5690\n",
      "Epoch 172/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.5688\n",
      "Epoch 173/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.5440\n",
      "Epoch 174/256\n",
      "6114/6114 [==============================] - 110s 18ms/sample - loss: 0.5316\n",
      "Epoch 175/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.5124\n",
      "Epoch 176/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.4934\n",
      "Epoch 177/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.4726\n",
      "Epoch 178/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.4658\n",
      "Epoch 179/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.4567\n",
      "Epoch 180/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.4539\n",
      "Epoch 181/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.4531\n",
      "Epoch 182/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.4558\n",
      "Epoch 183/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.4439\n",
      "Epoch 184/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.4159\n",
      "Epoch 185/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.3986\n",
      "Epoch 186/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.3829\n",
      "Epoch 187/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.3834\n",
      "Epoch 188/256\n",
      "6114/6114 [==============================] - 111s 18ms/sample - loss: 0.3748\n",
      "Epoch 189/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.3674\n",
      "Epoch 190/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.3549\n",
      "Epoch 191/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.3420\n",
      "Epoch 192/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.3395\n",
      "Epoch 193/256\n",
      "5120/6114 [========================>.....] - ETA: 16s - loss: 0.3364\n",
      "----- Generating text after Epoch: 192\n",
      "----- Generating with seed: \"e,\n",
      "Which though it alter not love's sole effect,\n",
      "Yet doth it steal sweet hours from love's delight.\n",
      "I may not evermore acknowledge thee,\n",
      "Lest my bewailed guilt should do thee shame,\n",
      "Nor thou with public kindness honour me,\n",
      "Unless thou take that honour from thy name:\n",
      "  But do not so, I love thee in such sort,\n",
      "  As thou being mine, mine is thy good report.\n",
      "\n",
      "\n",
      "                    37\n",
      "\n",
      "As a decrepit father takes delight,\n",
      "To see his active child do deeds of youth,\n",
      "So I, made lame by Fortune's dearest spite\n",
      "Take al\"\n",
      "e,\n",
      "Which though it alter not love's sole effect,\n",
      "Yet doth it steal sweet hours from love's delight.\n",
      "I may not evermore acknowledge thee,\n",
      "Lest my bewailed guilt should do thee shame,\n",
      "Nor thou with public kindness honour me,\n",
      "Unless thou take that honour from thy name:\n",
      "  But do not so, I love thee in such sort,\n",
      "  As thou being mine, mine is thy good report.\n",
      "\n",
      "\n",
      "                    37\n",
      "\n",
      "As a decrepit father takes delight,\n",
      "To see his active child do deeds of youth,\n",
      "So I, made lame by Fortune's dearest spite\n",
      "Take alfeen oo ffounded dod didrande\n",
      "Whan bee eee-jess mome me dall I f1alg,\n",
      "But waac on bean y y areed blime,\n",
      "Bat sild ain, or the kap\n",
      "6114/6114 [==============================] - 138s 23ms/sample - loss: 0.3385\n",
      "Epoch 194/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.3344\n",
      "Epoch 195/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.3095\n",
      "Epoch 196/256\n",
      "6114/6114 [==============================] - 113s 18ms/sample - loss: 0.3001\n",
      "Epoch 197/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2918\n",
      "Epoch 198/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.2838\n",
      "Epoch 199/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.2737\n",
      "Epoch 200/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2751\n",
      "Epoch 201/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2719\n",
      "Epoch 202/256\n",
      "6114/6114 [==============================] - 112s 18ms/sample - loss: 0.2583\n",
      "Epoch 203/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.2492\n",
      "Epoch 204/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2381\n",
      "Epoch 205/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2389\n",
      "Epoch 206/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2361\n",
      "Epoch 207/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.2224\n",
      "Epoch 208/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2123\n",
      "Epoch 209/256\n",
      "6114/6114 [==============================] - 113s 19ms/sample - loss: 0.2084\n",
      "Epoch 210/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2055\n",
      "Epoch 211/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2045\n",
      "Epoch 212/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.1919\n",
      "Epoch 213/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1914\n",
      "Epoch 214/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1874\n",
      "Epoch 215/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1966\n",
      "Epoch 216/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2171\n",
      "Epoch 217/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.2408\n",
      "Epoch 218/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2409\n",
      "Epoch 219/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2356\n",
      "Epoch 220/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2230\n",
      "Epoch 221/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1937\n",
      "Epoch 222/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1693\n",
      "Epoch 223/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1531\n",
      "Epoch 224/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.1447\n",
      "Epoch 225/256\n",
      "5120/6114 [========================>.....] - ETA: 16s - loss: 0.1380\n",
      "----- Generating text after Epoch: 224\n",
      "----- Generating with seed: \"not (love) disgrace me half so ill,\n",
      "To set a form upon desired change,\n",
      "As I'll my self disgrace, knowing thy will,\n",
      "I will acquaintance strangle and look strange:\n",
      "Be absent from thy walks and in my tongue,\n",
      "Thy sweet beloved name no more shall dwell,\n",
      "Lest I (too much profane) should do it wronk:\n",
      "And haply of our old acquaintance tell.\n",
      "  For thee, against my self I'll vow debate,\n",
      "  For I must ne'er love him whom thou dost hate.\n",
      "\n",
      "\n",
      "                    90\n",
      "\n",
      "Then hate me when thou wilt, if ever, now,\n",
      "Now while the \"\n",
      "not (love) disgrace me half so ill,\n",
      "To set a form upon desired change,\n",
      "As I'll my self disgrace, knowing thy will,\n",
      "I will acquaintance strangle and look strange:\n",
      "Be absent from thy walks and in my tongue,\n",
      "Thy sweet beloved name no more shall dwell,\n",
      "Lest I (too much profane) should do it wronk:\n",
      "And haply of our old acquaintance tell.\n",
      "  For thee, against my self I'll vow debate,\n",
      "  For I must ne'er love him whom thou dost hate.\n",
      "\n",
      "\n",
      "                    90\n",
      "\n",
      "Then hate me when thou wilt, if ever, now,\n",
      "Now while the brarrind berow the gors'spiesshingg minedacd\n",
      "\n",
      "A ' brarsf llfllromy muthtur arwirn shapeee epeelsssss,\n",
      "Dongng tuy fartt flerverod\n",
      "6114/6114 [==============================] - 141s 23ms/sample - loss: 0.1384\n",
      "Epoch 226/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1293\n",
      "Epoch 227/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.1219\n",
      "Epoch 228/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1189\n",
      "Epoch 229/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1227\n",
      "Epoch 230/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1246\n",
      "Epoch 231/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1148\n",
      "Epoch 232/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1090\n",
      "Epoch 233/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1043\n",
      "Epoch 234/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1025\n",
      "Epoch 235/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0989\n",
      "Epoch 236/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.0969\n",
      "Epoch 237/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.0980\n",
      "Epoch 238/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1494\n",
      "Epoch 239/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.2353\n",
      "Epoch 240/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2984\n",
      "Epoch 241/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.2854\n",
      "Epoch 242/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.2479\n",
      "Epoch 243/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.2242\n",
      "Epoch 244/256\n",
      "6114/6114 [==============================] - 117s 19ms/sample - loss: 0.2041\n",
      "Epoch 245/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.1749\n",
      "Epoch 246/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1464\n",
      "Epoch 247/256\n",
      "6114/6114 [==============================] - 116s 19ms/sample - loss: 0.1275\n",
      "Epoch 248/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1106\n",
      "Epoch 249/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.1026\n",
      "Epoch 250/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0909\n",
      "Epoch 251/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0839\n",
      "Epoch 252/256\n",
      "6114/6114 [==============================] - 114s 19ms/sample - loss: 0.0766\n",
      "Epoch 253/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0713\n",
      "Epoch 254/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0665\n",
      "Epoch 255/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0630\n",
      "Epoch 256/256\n",
      "6114/6114 [==============================] - 115s 19ms/sample - loss: 0.0596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2056da31a20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=1024,\n",
    "          epochs=256,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"y precious minutes waste,\n",
      "These vacant leaves thy mind's imprint will bear,\n",
      "And of this book, this learning mayst thou taste.\n",
      "The wrinkles which thy glass will truly show,\n",
      "Of mouthed graves will give thee memory,\n",
      "Thou by thy dial's shady stealth mayst know,\n",
      "Time's thievish progress to eternity.\n",
      "Look what thy memory cannot contain,\n",
      "Commit to these waste blanks, and thou shalt find\n",
      "Those children nursed, delivered from thy brain,\n",
      "To take a new acquaintance of thy mind.\n",
      "  These offices, so oft as thou wilt loo\"\n",
      "y precious minutes waste,\n",
      "These vacant leaves thy mind's imprint will bear,\n",
      "And of this book, this learning mayst thou taste.\n",
      "The wrinkles which thy glass will truly show,\n",
      "Of mouthed graves will give thee memory,\n",
      "Thou by thy dial's shady stealth mayst know,\n",
      "Time's thievish progress to eternity.\n",
      "Look what thy memory cannot contain,\n",
      "Commit to these waste blanks, and thou shalt find\n",
      "Those children nursed, delivered from thy brain,\n",
      "To take a new acquaintance of thy mind.\n",
      "  These offices, so oft as thou wilt look,\n",
      "The  beapt o  berksood be  oute pliee,\n",
      "\n",
      "\n",
      "A   Sbe  b be ly fakny fube y aderew oo thin spat,\n",
      "Rieurriqfartle j ssamysing goound ho  hpperich,\n",
      "For which shyurrut ou hald dich thamemy,\n",
      "And the eoul piissing thy aid dor will fhettou' pinnse.\n",
      "\n",
      "AC And be bellllkssstuth thye'samy'rind th weel  bell-coqmyyixg?\n",
      "Woing ho sialt, tit houd ow  hou shis, oflltee,\n",
      "Or Iass ssppirsaadd hith lllsrirt,\n",
      "Th thom warr ho drirf yove  tee.\n",
      "\n",
      "\n",
      "                   7e 136\n",
      "BAt duth bu use, hade I I be ived couth y pere,\n",
      "Wheccat oun this spest, hetdarr win spess,\n",
      "  he beanny ho lltees, I netus untenow\n",
      " h thee peee,\n",
      " ABrtacting wiin worth loveroo  or theene.\n",
      "\n",
      "\n",
      " Y     h             21\n",
      "\n",
      " oout'' pacen, kalle, any of meartredoo  wee o eave,\n",
      "Mysund illfarrcand mind art ou I er thet me pite,\n",
      "But nut  ow plots'sthingd in wint th tarcmy my imt.\n",
      "The  oe fhe fhes poo found erow' roo fomy ee.\n",
      "\n",
      "\n",
      " o   Suth  be  o  ore hoves, Acd fuccamynedecowrsst se pepimeenerowr.\n",
      "\n",
      " I  sh be  be becckaccowit  m  ialkainges orencccancamy sine,\n",
      "O theceve'sthouh meaberridd aowh te eleve,, hinggettinn wh  o beillost, ave thy cammdy\n",
      "Wh s ee beqult mund beand buca kne wmatt,\n",
      "Than  aalb'qutty s om mesthing wurtut  eillity fyorg,\n",
      "no dwint  fealllofreros  heetree vee,\n",
      "Whecchgcong whon  oubearnd ach beante ixsssesssheee:\n",
      "Soum mu e tial, and tut  ou ho kekes,\n",
      "Toouggttunn nwwwill ssppitt, ho dadr an wwils Ingh)swaren I seareneasNed,\n",
      "u  he bbe k 0elll- faigtyy h diir dorrenn\n",
      "Whe.  he berute thece oo metpee,\n",
      "s Thaug zubaar willl sf mmy uname heldverd,\n",
      "But aud th mearqriity th ther arrerespriss,\n",
      "Fout musut thin somminced coow'ty I pepesso,\n",
      "I daduauu y ind ill arty ul 1eve,\n",
      "A\n",
      "For thee ty ucaan o  mellaved,\n",
      "A dvedimught connwr wh th thees brured,\n",
      "Foopgreckndaws whe bells'srof be berte.\n",
      "B orenfus sf beam's balddido woorthe peere:\n",
      "No owml's Issss inggetthind in werert seccccannnnwwrr,  hyeedgrarddduth h sbeissd hime mint.\n",
      "O  plove and mald ind ixppanteed ive,\n",
      "Or ccaccthin wir wot  oa this doomt enavy,\n",
      "W itt thate manne ow sorss the fearrero\n",
      "g thouggt, and tho hom berrdo ho lever,\n",
      "A d balld\n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "generated = ''\n",
    "\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(2048):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_int[char]] = 1\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature=1.0)\n",
    "    next_char = int_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

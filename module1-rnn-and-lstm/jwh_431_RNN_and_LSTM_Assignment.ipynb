{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "# r = requests.get(url)\n",
    "# r.encoding = r.apparent_encoding\n",
    "# data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    txt_data.splitlines()\n",
    "    txt_data.rstrip('\\n') # didnt work, but maybe it's better with them\n",
    "    txt_data.split('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_data = open('sonnets.txt', 'r').read() # test external files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = list(set(txt_data))\n",
    "# num_chars = len(chars)\n",
    "# txt_data_size = len(txt_data)\n",
    "# print(f'Total characters: {txt_data_size}. Unique characters: {num_chars}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# integer_encoded = [char_to_int[i] for i in txt_data] # could be used for manual one-hot encode\n",
    "# print(int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually copied the block of 154 sonnets into notepad, then used find and replace to make the fancy apostrophes a normal character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sonnets.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text.split('\\n\\n\\n')[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98328"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [char_int[c] for c in text]\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 98264\n"
     ]
    }
   ],
   "source": [
    "maxlen = 64\n",
    "step = 4\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((98264, 64, 71), (98264, 71))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 22:43:56.395386 11536 deprecation.py:506] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(128):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.8604\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" shall burn:\n",
      "The living record of your memory.\n",
      "'Gainst death, an\"\n",
      " shall burn:\n",
      "The living record of your memory.\n",
      "'Gainst death, an whet maye the il kyoug,\n",
      "Wnonnow loventhy so beare,\n",
      "Of him I you\n",
      "98264/98264 [==============================] - 56s 566us/sample - loss: 1.8604\n",
      "Epoch 2/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.8157\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"Those lines that I before have writ do lie,\n",
      "Even those that said\"\n",
      "Those lines that I before have writ do lie,\n",
      "Even those that saidet in puay bond,\n",
      "  There bet thy lay, the sichnfeven in and lise\n",
      "98264/98264 [==============================] - 56s 569us/sample - loss: 1.8157\n",
      "Epoch 3/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.7799\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"me with that sun thine eye,\n",
      "When love converted from the thing i\"\n",
      "me with that sun thine eye,\n",
      "When love converted from the thing ic dile caine,\n",
      "And ow lompurlantientente the llopgrese onelenf\n",
      "An\n",
      "98264/98264 [==============================] - 56s 568us/sample - loss: 1.7797\n",
      "Epoch 4/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.7477\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"        133\n",
      "\n",
      "Beshrew that heart that makes my heart to groan\n",
      "For\"\n",
      "        133\n",
      "\n",
      "Beshrew that heart that makes my heart to groan\n",
      "Forkartaty pare, And andss my dee?\n",
      "They my dew rlace,\n",
      "Whot arsenten\n",
      "98264/98264 [==============================] - 56s 575us/sample - loss: 1.7474\n",
      "Epoch 5/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.7181\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"lone,\n",
      "Sinks down to death, oppressed with melancholy.\n",
      "Until life\"\n",
      "lone,\n",
      "Sinks down to death, oppressed with melancholy.\n",
      "Until life ttaye his destored on that 'ngaser,\n",
      "Thy thy wruve's rage.' the \n",
      "98264/98264 [==============================] - 56s 569us/sample - loss: 1.7181\n",
      "Epoch 6/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.6928\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"a happy title do I find,\n",
      "Happy to have thy love, happy to die!\n",
      " \"\n",
      "a happy title do I find,\n",
      "Happy to have thy love, happy to die!\n",
      "  Is byanted of all essins reme anjaye,\n",
      "And thy sulary lovence ho\n",
      "98264/98264 [==============================] - 56s 568us/sample - loss: 1.6927\n",
      "Epoch 7/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.6700\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"                  144\n",
      "\n",
      "Two loves I have of comfort and despair,\n",
      "\"\n",
      "                  144\n",
      "\n",
      "Two loves I have of comfort and despair,\n",
      "Thou vill ast, whing the inay kenkes self-seare.\n",
      "  So eries wort\n",
      "98264/98264 [==============================] - 56s 566us/sample - loss: 1.6699\n",
      "Epoch 8/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.6494\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \" a son.\n",
      "\n",
      "\n",
      "                    8\n",
      "\n",
      "Music to hear, why hear'st thou\"\n",
      " a son.\n",
      "\n",
      "\n",
      "                    8\n",
      "\n",
      "Music to hear, why hear'st thougit on he,\n",
      "That prubse, wht in sther whate groul fibless:\n",
      "The to\n",
      "98264/98264 [==============================] - 56s 574us/sample - loss: 1.6494\n",
      "Epoch 9/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.6293\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"\n",
      "And Will to boot, and Will in overplus,\n",
      "More than enough am I t\"\n",
      "\n",
      "And Will to boot, and Will in overplus,\n",
      "More than enough am I thighcaso in my lurd.\n",
      "\n",
      "\n",
      "                    108\n",
      "\n",
      "For anthes thou \n",
      "98264/98264 [==============================] - 56s 573us/sample - loss: 1.6293\n",
      "Epoch 10/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.6132\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"ks in, like the dyer's hand:\n",
      "Pity me then, and wish I were renew\"\n",
      "ks in, like the dyer's hand:\n",
      "Pity me then, and wish I were renewes to gree,\n",
      "As being these-sur her I have reauted\n",
      "Thing ene, I r\n",
      "98264/98264 [==============================] - 56s 573us/sample - loss: 1.6132\n",
      "Epoch 11/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5953\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \" worst to steal thy self away,\n",
      "For term of life thou art assured\"\n",
      " worst to steal thy self away,\n",
      "For term of life thou art assureded of yece,\n",
      "Wire that hover me ngreinures beazed,\n",
      "And shows ab a\n",
      "98264/98264 [==============================] - 56s 574us/sample - loss: 1.5957\n",
      "Epoch 12/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5797\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \"ing sun of heaven\n",
      "Better becomes the grey cheeks of the east,\n",
      "No\"\n",
      "ing sun of heaven\n",
      "Better becomes the grey cheeks of the east,\n",
      "Nor mave lives the wily eivow when than thuse.\n",
      "\n",
      "\n",
      "                 \n",
      "98264/98264 [==============================] - 56s 574us/sample - loss: 1.5797\n",
      "Epoch 13/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5645\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"rit do lie,\n",
      "Even those that said I could not love you dearer,\n",
      "Ye\"\n",
      "rit do lie,\n",
      "Even those that said I could not love you dearer,\n",
      "Yet sungith thou then beauty of all strengle?\n",
      "O levire not that ma\n",
      "98264/98264 [==============================] - 57s 581us/sample - loss: 1.5643\n",
      "Epoch 14/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5503\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"nd you on the top of happy hours,\n",
      "And many maiden gardens yet un\"\n",
      "nd you on the top of happy hours,\n",
      "And many maiden gardens yet undo fice,\n",
      "Soetmress I sis posething a mence be,\n",
      "  In times frife \n",
      "98264/98264 [==============================] - 57s 577us/sample - loss: 1.5504\n",
      "Epoch 15/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5382\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \" age to come would say this poet lies,\n",
      "Such heavenly touches ne'\"\n",
      " age to come would say this poet lies,\n",
      "Such heavenly touches ne's bokent, ars whemised with toon:\n",
      "When the ofterrer gake his gro\n",
      "98264/98264 [==============================] - 56s 573us/sample - loss: 1.5381\n",
      "Epoch 16/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5246\n",
      "----- Generating text after Epoch: 15\n",
      "----- Generating with seed: \" children's eyes, her husband's shape in mind:\n",
      "Look what an unth\"\n",
      " children's eyes, her husband's shape in mind:\n",
      "Look what an unther distecrermine.\n",
      "\n",
      "\n",
      "                    147\n",
      "\n",
      "So primion exes uno\n",
      "98264/98264 [==============================] - 57s 576us/sample - loss: 1.5245\n",
      "Epoch 17/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5126\n",
      "----- Generating text after Epoch: 16\n",
      "----- Generating with seed: \"d annexed thy breath,\n",
      "But for his theft in pride of all his grow\"\n",
      "d annexed thy breath,\n",
      "But for his theft in pride of all his grow?\n",
      "Whoue a know recuse, thy mieth thise cellod hame,\n",
      "The mooks ev\n",
      "98264/98264 [==============================] - 56s 572us/sample - loss: 1.5129\n",
      "Epoch 18/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.5015\n",
      "----- Generating text after Epoch: 17\n",
      "----- Generating with seed: \"es and your gifts to tell.\n",
      "  And more, much more than in my vers\"\n",
      "es and your gifts to tell.\n",
      "  And more, much more than in my verse shame thing she rushe,\n",
      "To thy incerpeer arw of evinace,\n",
      "On, mo\n",
      "98264/98264 [==============================] - 56s 573us/sample - loss: 1.5014\n",
      "Epoch 19/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4886- ETA: 0s - loss: 1.\n",
      "----- Generating text after Epoch: 18\n",
      "----- Generating with seed: \" sweet issue your sweet form should bear.\n",
      "Who lets so fair a hou\"\n",
      " sweet issue your sweet form should bear.\n",
      "Who lets so fair a hours what these eartence can.\n",
      "  But but not will, Abjuble it my gr\n",
      "98264/98264 [==============================] - 57s 575us/sample - loss: 1.4887\n",
      "Epoch 20/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4791\n",
      "----- Generating text after Epoch: 19\n",
      "----- Generating with seed: \"dies dead, and lovely knights,\n",
      "Then in the blazon of sweet beaut\"\n",
      "dies dead, and lovely knights,\n",
      "Then in the blazon of sweet beauty aptilly ill,\n",
      "And I may dight of filwen's meckout alt,\n",
      "But thy \n",
      "98264/98264 [==============================] - 56s 574us/sample - loss: 1.4793\n",
      "Epoch 21/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4673\n",
      "----- Generating text after Epoch: 20\n",
      "----- Generating with seed: \"times happier be it ten for one,\n",
      "Ten times thy self were happier\"\n",
      "times happier be it ten for one,\n",
      "Ten times thy self were happier, and thou ise,\n",
      "Thy inore bu, wordd speed not tranking one.\n",
      "\n",
      "\n",
      "  \n",
      "98264/98264 [==============================] - 56s 569us/sample - loss: 1.4673\n",
      "Epoch 22/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4576\n",
      "----- Generating text after Epoch: 21\n",
      "----- Generating with seed: \"                   120\n",
      "\n",
      "That you were once unkind befriends me n\"\n",
      "                   120\n",
      "\n",
      "That you were once unkind befriends me nurk.\n",
      "So dreak my lead hat greep I the may\n",
      "For shall love thee fo\n",
      "98264/98264 [==============================] - 56s 571us/sample - loss: 1.4577\n",
      "Epoch 23/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4467\n",
      "----- Generating text after Epoch: 22\n",
      "----- Generating with seed: \"ss lively heat still to endure,\n",
      "And grew a seeting bath which ye\"\n",
      "ss lively heat still to endure,\n",
      "And grew a seeting bath which yet breakest.\n",
      "\n",
      "\n",
      "                    31\n",
      "\n",
      "An what friess for a hand \n",
      "98264/98264 [==============================] - 56s 570us/sample - loss: 1.4466\n",
      "Epoch 24/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4385\n",
      "----- Generating text after Epoch: 23\n",
      "----- Generating with seed: \"ut difference.\n",
      "Fair, kind, and true, is all my argument,\n",
      "Fair, k\"\n",
      "ut difference.\n",
      "Fair, kind, and true, is all my argument,\n",
      "Fair, know this policy not the reigly) yiss,\n",
      "Corvont my ment thine not \n",
      "98264/98264 [==============================] - 56s 571us/sample - loss: 1.4382\n",
      "Epoch 25/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4294\n",
      "----- Generating text after Epoch: 24\n",
      "----- Generating with seed: \"to sell.\n",
      "\n",
      "\n",
      "                    22\n",
      "\n",
      "My glass shall not persuade m\"\n",
      "to sell.\n",
      "\n",
      "\n",
      "                    22\n",
      "\n",
      "My glass shall not persuade my love a did,\n",
      "Seemented of being breaking thee sige,\n",
      "Is nome des\n",
      "98264/98264 [==============================] - 56s 572us/sample - loss: 1.4293\n",
      "Epoch 26/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4193\n",
      "----- Generating text after Epoch: 25\n",
      "----- Generating with seed: \"oms have full as deep a dye,\n",
      "As the perfumed tincture of the ros\"\n",
      "oms have full as deep a dye,\n",
      "As the perfumed tincture of the rose.\n",
      "\n",
      "\n",
      "                    138\n",
      "\n",
      "I dou my do forim bethan thichelin\n",
      "98264/98264 [==============================] - 57s 581us/sample - loss: 1.4193\n",
      "Epoch 27/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4112\n",
      "----- Generating text after Epoch: 26\n",
      "----- Generating with seed: \"us, mine eye's due is thy outward part,\n",
      "  And my heart's right, \"\n",
      "us, mine eye's due is thy outward part,\n",
      "  And my heart's right, and this whyod, the sidurn)\n",
      "Nor did posectarly, sell be time\n",
      "In \n",
      "98264/98264 [==============================] - 57s 578us/sample - loss: 1.4112\n",
      "Epoch 28/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.4026\n",
      "----- Generating text after Epoch: 27\n",
      "----- Generating with seed: \" For thee, and for my self, no quiet find.\n",
      "\n",
      "\n",
      "                   \"\n",
      " For thee, and for my self, no quiet find.\n",
      "\n",
      "\n",
      "                    62\n",
      "\n",
      "That no hut I wilt to am the stare to greed,\n",
      "And one is thi\n",
      "98264/98264 [==============================] - 57s 577us/sample - loss: 1.4026\n",
      "Epoch 29/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.3950\n",
      "----- Generating text after Epoch: 28\n",
      "----- Generating with seed: \"aste or ruining?\n",
      "Have I not seen dwellers on form and favour\n",
      "Los\"\n",
      "aste or ruining?\n",
      "Have I not seen dwellers on form and favour\n",
      "Losing his so sake of anoner tere,\n",
      "That to thy musars she swarthers\n",
      "98264/98264 [==============================] - 57s 578us/sample - loss: 1.3949\n",
      "Epoch 30/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.3861\n",
      "----- Generating text after Epoch: 29\n",
      "----- Generating with seed: \"y days,\n",
      "(Making lascivious comments on thy sport)\n",
      "Cannot disprai\"\n",
      "y days,\n",
      "(Making lascivious comments on thy sport)\n",
      "Cannot dispraint not so long sicthoun strings,\n",
      "By anies that should not since \n",
      "98264/98264 [==============================] - 57s 578us/sample - loss: 1.3859\n",
      "Epoch 31/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.3786\n",
      "----- Generating text after Epoch: 30\n",
      "----- Generating with seed: \"d that in guess they measure by thy deeds,\n",
      "Then churls their tho\"\n",
      "d that in guess they measure by thy deeds,\n",
      "Then churls their though the hank's treeps never,\n",
      "To mone oon mowel is tomendly wendo\n",
      "98264/98264 [==============================] - 57s 576us/sample - loss: 1.3785\n",
      "Epoch 32/32\n",
      "98176/98264 [============================>.] - ETA: 0s - loss: 1.3712\n",
      "----- Generating text after Epoch: 31\n",
      "----- Generating with seed: \"resh numbers number all your graces,\n",
      "The age to come would say t\"\n",
      "resh numbers number all your graces,\n",
      "The age to come would say to be forth,\n",
      "I frownts no unimst on that collamy him dead\n",
      "Unlesiq\n",
      "98264/98264 [==============================] - 57s 577us/sample - loss: 1.3711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28db2d62b38>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=32,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"t not, nor he will not be free,\n",
      "For thou art covetous, and he is\"\n",
      "t not, nor he will not be free,\n",
      "For thou art covetous, and he is cuerift,\n",
      "That taking no am hate of bear ills grow,\n",
      "Peiting over's every notly right lovely,\n",
      "Shold duephous duth that rasicty you doth mightnece,\n",
      "By thou iprift but the pleak yetw action,\n",
      "To hat theum love is mirenkshill O'trood:\n",
      "One happy in misher's brock I not old,\n",
      "That is my jays that live if store dear by\n",
      "Those heart whate oly beauty ot shald this trope:\n",
      "  Then raving purvough that shouls was have years,\n",
      "  And that mank wo how ingild wrulds womlly?\n",
      "Fur am deep a beauty's gonetest's b:\n",
      "Wilt thee oncrively self alate that both.\n",
      "\n",
      "\n",
      "                    44\n",
      "\n",
      "Bot fall staiv'st of summer the taty\n",
      "Groal gutst fast dead strauge Eve shame ded,\n",
      "Mideau, sweet strains at elforn see their geld,\n",
      "  Kill mixe, that follows you best take her thy live.\n",
      "  For ous not love shade, all that wert feld your wait:\n",
      "Hank every your sweet bectioved forthich hipt,\n",
      "And be sifful hides I dear brounts to grave,\n",
      "Thy wayer that a ear'y it the proviogn,\n",
      "Thou are rild meed ppiction new love thought,\n",
      "Lest paript to thue at state more thou are make:\n",
      "Then time, 'Tis thus illscanents by not welt:\n",
      "  And theasure a wathen wrets trifch I most drough:\n",
      "And you she wastel the meraupy's gild?\n",
      "My night of this are know hat grown and year,\n",
      "Times thou baintles of with the tell,\n",
      "  Gays she longly peementized would hast deceeds,\n",
      "I lets tere is partate thee haply pired,\n",
      "With they duth warthoy virtay to chail;\n",
      "That probe in this self-coeptor's grause thee.\n",
      "Since pait out dost by thy sall retiment,\n",
      "By thrigh in the with more true shall his pift,\n",
      "My thought he world curnid diftiours any sinf\n",
      "With cold be forse, light, and Ily to hark.\n",
      "The withar of-baintate this ightroncespled\n",
      "Lith tenceury behoff-excelts still.\n",
      "\n",
      "\n",
      "                    108\n",
      "\n",
      "Making wood gaze thrieed (with if but my frail.\n",
      "  Or hast acrevide the with my love talen like,\n",
      "Like the time that thine eyes hadd faoker,\n",
      "Whoke mide will ofther, and with that mine yoly\n",
      "Have leved to beautit orher which edlivichalling:\n",
      "  Yet impoctule sweet I chide thou bore may\n",
      "Mades to heatter slem they self richinite,\n",
      "Dissolf c\n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "generated = ''\n",
    "\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(2048):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_int[char]] = 1\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature=1.0)\n",
    "    next_char = int_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('plays.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [char_int[c] for c in text]\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 69709\n"
     ]
    }
   ],
   "source": [
    "maxlen = 64\n",
    "step = 16\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69709, 64, 65), (69709, 65))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 23:57:08.525154  8588 deprecation.py:506] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "        \n",
    "    if epoch % 16 == 0:\n",
    "    \n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        generated = ''\n",
    "\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(128):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature=1.0)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0127 23:57:08.845052  8588 deprecation.py:323] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 3.0081\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"uch liberty, my Lucio, liberty:\n",
      "As surfeit is the father of much\"\n",
      "uch liberty, my Lucio, liberty:\n",
      "As surfeit is the father of muche aslcle he ges sertis thsrmoad besite gna thl hos the ap chelt.\n",
      "\n",
      " pRDian:\n",
      "\n",
      "Chemedc eoles mios .\n",
      "\n",
      "ShaisEwhipuer, uinh NhC atd ci\n",
      "69709/69709 [==============================] - 42s 600us/sample - loss: 3.0077\n",
      "Epoch 2/128\n",
      "69709/69709 [==============================] - 40s 580us/sample - loss: 2.4234\n",
      "Epoch 3/128\n",
      "69709/69709 [==============================] - 44s 628us/sample - loss: 2.2726\n",
      "Epoch 4/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 2.1809\n",
      "Epoch 5/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 2.1093\n",
      "Epoch 6/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 2.0476\n",
      "Epoch 7/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 1.9997\n",
      "Epoch 8/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 1.9574\n",
      "Epoch 9/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 1.9178\n",
      "Epoch 10/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 1.8834\n",
      "Epoch 11/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 1.8510\n",
      "Epoch 12/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 1.8219\n",
      "Epoch 13/128\n",
      "69709/69709 [==============================] - 44s 630us/sample - loss: 1.7932\n",
      "Epoch 14/128\n",
      "69709/69709 [==============================] - 44s 625us/sample - loss: 1.7677\n",
      "Epoch 15/128\n",
      "69709/69709 [==============================] - 44s 629us/sample - loss: 1.7440\n",
      "Epoch 16/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 1.7196\n",
      "Epoch 17/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 1.6977\n",
      "----- Generating text after Epoch: 16\n",
      "----- Generating with seed: \" look pale--they threw their caps\n",
      "As they would hang them on the\"\n",
      " look pale--they threw their caps\n",
      "As they would hang them on the shull\n",
      "A jay ctuster, tnoubun, Earcled bet is cuspel\n",
      "Hei''d meker a dorkill to hath soor wich seupt\n",
      "Us a bue. is are well be net\n",
      "69709/69709 [==============================] - 45s 653us/sample - loss: 1.6979\n",
      "Epoch 18/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 1.6756\n",
      "Epoch 19/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 1.6548\n",
      "Epoch 20/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 1.6361\n",
      "Epoch 21/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 1.6158\n",
      "Epoch 22/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 1.5977\n",
      "Epoch 23/128\n",
      "69709/69709 [==============================] - 45s 642us/sample - loss: 1.5785\n",
      "Epoch 24/128\n",
      "69709/69709 [==============================] - 45s 641us/sample - loss: 1.5602\n",
      "Epoch 25/128\n",
      "69709/69709 [==============================] - 45s 647us/sample - loss: 1.5420\n",
      "Epoch 26/128\n",
      "69709/69709 [==============================] - 45s 642us/sample - loss: 1.5248\n",
      "Epoch 27/128\n",
      "69709/69709 [==============================] - 45s 639us/sample - loss: 1.5064\n",
      "Epoch 28/128\n",
      "69709/69709 [==============================] - 42s 599us/sample - loss: 1.4910\n",
      "Epoch 29/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.4742\n",
      "Epoch 30/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.4553\n",
      "Epoch 31/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.4401\n",
      "Epoch 32/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.4244\n",
      "Epoch 33/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 1.4081\n",
      "----- Generating text after Epoch: 32\n",
      "----- Generating with seed: \" the Duke of York.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Richard of York! how fares ou\"\n",
      " the Duke of York.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Richard of York! how fares our druponut this?\n",
      "\n",
      "PEsevES:\n",
      "When, my raids, and he shall wear hath be,\n",
      "O, wear' lack; and an a ghald not beef to godath,\n",
      "Agaiest \n",
      "69709/69709 [==============================] - 40s 574us/sample - loss: 1.4082\n",
      "Epoch 34/128\n",
      "69709/69709 [==============================] - 40s 571us/sample - loss: 1.3922\n",
      "Epoch 35/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.3766\n",
      "Epoch 36/128\n",
      "69709/69709 [==============================] - 39s 559us/sample - loss: 1.3614\n",
      "Epoch 37/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.3445\n",
      "Epoch 38/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.3288\n",
      "Epoch 39/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.3126\n",
      "Epoch 40/128\n",
      "69709/69709 [==============================] - 39s 558us/sample - loss: 1.2961\n",
      "Epoch 41/128\n",
      "69709/69709 [==============================] - 39s 559us/sample - loss: 1.2797\n",
      "Epoch 42/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.2667\n",
      "Epoch 43/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.2510\n",
      "Epoch 44/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.2378\n",
      "Epoch 45/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.2209\n",
      "Epoch 46/128\n",
      "69709/69709 [==============================] - 39s 560us/sample - loss: 1.2061\n",
      "Epoch 47/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.1924\n",
      "Epoch 48/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.1810\n",
      "Epoch 49/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 1.1667\n",
      "----- Generating text after Epoch: 48\n",
      "----- Generating with seed: \"ee by Rosaline's bright eyes,\n",
      "By her high forehead and her scarl\"\n",
      "ee by Rosaline's bright eyes,\n",
      "By her high forehead and her scarly; and ganely.\n",
      "Wamis est upon the have neiror all with as\n",
      "I hil deat to you, faill we may pawn be me.\n",
      "Goou, for the man I as wea\n",
      "69709/69709 [==============================] - 40s 572us/sample - loss: 1.1667\n",
      "Epoch 50/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 1.1532\n",
      "Epoch 51/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.1390\n",
      "Epoch 52/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.1317\n",
      "Epoch 53/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 1.1143\n",
      "Epoch 54/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 1.1021\n",
      "Epoch 55/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 1.0897\n",
      "Epoch 56/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 1.0791\n",
      "Epoch 57/128\n",
      "69709/69709 [==============================] - 43s 618us/sample - loss: 1.0670\n",
      "Epoch 58/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 1.0629\n",
      "Epoch 59/128\n",
      "69709/69709 [==============================] - 44s 634us/sample - loss: 1.0471\n",
      "Epoch 60/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 1.0356\n",
      "Epoch 61/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 1.0290\n",
      "Epoch 62/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 1.0178\n",
      "Epoch 63/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 1.0094\n",
      "Epoch 64/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 0.9994\n",
      "Epoch 65/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 0.9938\n",
      "----- Generating text after Epoch: 64\n",
      "----- Generating with seed: \"\n",
      "To speak of peace or war.\n",
      "I talk of you:\n",
      "Why did you wish me mi\"\n",
      "\n",
      "To speak of peace or war.\n",
      "I talk of you:\n",
      "Why did you wish me misst here yeir to dlem,\n",
      "And then exconn's theart the jumbory.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I do such noble did the dukine the genifter\n",
      "Than \n",
      "69709/69709 [==============================] - 45s 652us/sample - loss: 0.9939\n",
      "Epoch 66/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.9809\n",
      "Epoch 67/128\n",
      "69709/69709 [==============================] - 45s 639us/sample - loss: 0.9763\n",
      "Epoch 68/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.9647\n",
      "Epoch 69/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.9582\n",
      "Epoch 70/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.9531\n",
      "Epoch 71/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.9471\n",
      "Epoch 72/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.9366\n",
      "Epoch 73/128\n",
      "69709/69709 [==============================] - 45s 638us/sample - loss: 0.9328\n",
      "Epoch 74/128\n",
      "69709/69709 [==============================] - 45s 646us/sample - loss: 0.9251\n",
      "Epoch 75/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 0.9213\n",
      "Epoch 76/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.9119\n",
      "Epoch 77/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.9084\n",
      "Epoch 78/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.9033\n",
      "Epoch 79/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.8957\n",
      "Epoch 80/128\n",
      "69709/69709 [==============================] - 45s 641us/sample - loss: 0.8891\n",
      "Epoch 81/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 0.8830\n",
      "----- Generating text after Epoch: 80\n",
      "----- Generating with seed: \"th Warwick.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Heavens grant that Warwick's words \"\n",
      "th Warwick.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Heavens grant that Warwick's words sworl then\n",
      "Yow prove probld whole parbon and doe twell,\n",
      "With they magbils'd upout Cimiss bors\n",
      "The exfrimes advecury Had! I chast\n",
      "69709/69709 [==============================] - 45s 652us/sample - loss: 0.8830\n",
      "Epoch 82/128\n",
      "69709/69709 [==============================] - 45s 641us/sample - loss: 0.8853\n",
      "Epoch 83/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 0.8726\n",
      "Epoch 84/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.8807\n",
      "Epoch 85/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.8682\n",
      "Epoch 86/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.8811\n",
      "Epoch 87/128\n",
      "69709/69709 [==============================] - 45s 642us/sample - loss: 0.8603\n",
      "Epoch 88/128\n",
      "69709/69709 [==============================] - 44s 636us/sample - loss: 0.8501\n",
      "Epoch 89/128\n",
      "69709/69709 [==============================] - 45s 639us/sample - loss: 0.8502\n",
      "Epoch 90/128\n",
      "69709/69709 [==============================] - 44s 635us/sample - loss: 0.8434\n",
      "Epoch 91/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.8495\n",
      "Epoch 92/128\n",
      "69709/69709 [==============================] - 44s 628us/sample - loss: 0.8513\n",
      "Epoch 93/128\n",
      "69709/69709 [==============================] - 44s 631us/sample - loss: 0.8502\n",
      "Epoch 94/128\n",
      "69709/69709 [==============================] - 44s 637us/sample - loss: 0.8312\n",
      "Epoch 95/128\n",
      "69709/69709 [==============================] - 45s 639us/sample - loss: 0.8268\n",
      "Epoch 96/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.8240\n",
      "Epoch 97/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 0.8269\n",
      "----- Generating text after Epoch: 96\n",
      "----- Generating with seed: \" is,\n",
      "And I am tied to be obedient;\n",
      "For so your father charged me\"\n",
      " is,\n",
      "And I am tied to be obedient;\n",
      "For so your father charged me never comfon\n",
      "Appoietait; Glavair, ammourdigge a mannount,\n",
      "Aswence to create.\n",
      "\n",
      "POLINGENSBE:\n",
      "Well, Romen HHene dreather to loigme\n",
      "69709/69709 [==============================] - 46s 653us/sample - loss: 0.8267\n",
      "Epoch 98/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.8126\n",
      "Epoch 99/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.8176\n",
      "Epoch 100/128\n",
      "69709/69709 [==============================] - 45s 640us/sample - loss: 0.8121\n",
      "Epoch 101/128\n",
      "69709/69709 [==============================] - 45s 643us/sample - loss: 0.8088\n",
      "Epoch 102/128\n",
      "69709/69709 [==============================] - 45s 642us/sample - loss: 0.8046\n",
      "Epoch 103/128\n",
      "69709/69709 [==============================] - 45s 639us/sample - loss: 0.8005\n",
      "Epoch 104/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.8029\n",
      "Epoch 105/128\n",
      "69709/69709 [==============================] - 44s 638us/sample - loss: 0.7917\n",
      "Epoch 106/128\n",
      "69709/69709 [==============================] - 42s 605us/sample - loss: 0.7867\n",
      "Epoch 107/128\n",
      "69709/69709 [==============================] - 40s 579us/sample - loss: 0.7876\n",
      "Epoch 108/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 0.7845\n",
      "Epoch 109/128\n",
      "69709/69709 [==============================] - 40s 573us/sample - loss: 0.7872\n",
      "Epoch 110/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.8039\n",
      "Epoch 111/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.7826\n",
      "Epoch 112/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.7841\n",
      "Epoch 113/128\n",
      "69632/69709 [============================>.] - ETA: 0s - loss: 0.7670\n",
      "----- Generating text after Epoch: 112\n",
      "----- Generating with seed: \"le's best for winter: I have one\n",
      "Of sprites and goblins.\n",
      "\n",
      "HERMIO\"\n",
      "le's best for winter: I have one\n",
      "Of sprites and goblins.\n",
      "\n",
      "HERMIO:\n",
      "Not not fisll and Hadous, le make be noth, furs\n",
      "you lesserr hy sours we man read word to this band\n",
      "Drisher'd ulonk, me all req\n",
      "69709/69709 [==============================] - 40s 577us/sample - loss: 0.7670\n",
      "Epoch 114/128\n",
      "69709/69709 [==============================] - 39s 564us/sample - loss: 0.7673\n",
      "Epoch 115/128\n",
      "69709/69709 [==============================] - 39s 566us/sample - loss: 0.7656\n",
      "Epoch 116/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 0.7775\n",
      "Epoch 117/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.7704\n",
      "Epoch 118/128\n",
      "69709/69709 [==============================] - 39s 561us/sample - loss: 0.7584\n",
      "Epoch 119/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 0.7520\n",
      "Epoch 120/128\n",
      "69709/69709 [==============================] - 39s 564us/sample - loss: 0.7576\n",
      "Epoch 121/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 0.7542\n",
      "Epoch 122/128\n",
      "69709/69709 [==============================] - 39s 565us/sample - loss: 0.7468\n",
      "Epoch 123/128\n",
      "69709/69709 [==============================] - 39s 565us/sample - loss: 0.7504\n",
      "Epoch 124/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.7508\n",
      "Epoch 125/128\n",
      "69709/69709 [==============================] - 39s 563us/sample - loss: 0.7501\n",
      "Epoch 126/128\n",
      "69709/69709 [==============================] - 39s 564us/sample - loss: 0.7335\n",
      "Epoch 127/128\n",
      "69709/69709 [==============================] - 39s 564us/sample - loss: 0.7433\n",
      "Epoch 128/128\n",
      "69709/69709 [==============================] - 39s 562us/sample - loss: 0.7473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24b398a70b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=128,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"las! I have show'd too much\n",
      "The rashness of a woman: he is touch\"\n",
      "las! I have show'd too much\n",
      "The rashness of a woman: he is toucher and you:\n",
      "That thou rampores-baved it so deaf vouter.\n",
      "\n",
      "SICINIS:\n",
      "Sir, been sir? I am fullinessor:\n",
      "Let this assition.\n",
      "\n",
      "Lord: re'lood he rodory.\n",
      "\n",
      "MartHNS'S:\n",
      "\n",
      "PAMIST:\n",
      "Were is seapter me,\n",
      "Why tadest one doy is gesare\n",
      "In houses time in a many doung talk'd me?\n",
      "When, then will queen and the boun, sir,'t;\n",
      "Browerd good with a sable. Lord is his triafester,\n",
      "ED foogow cotciatu.\n",
      "\n",
      "NoClloon:\n",
      "my tooad's bese.\n",
      "\n",
      "ISCAMENS:\n",
      "Sire sonce I sting a that exely that change to your?\n",
      "\n",
      "Throwo ringranvermane:\n",
      "He ramed, good me thy grach his blood towrad:\n",
      "For what noth bait butnist grant, brother.\n",
      "\n",
      "DUCHORIO:\n",
      "I what's every hear neverest dishier to his poste,\n",
      "Firen, they streatter trabous from fliar no\n",
      "Wid, beith their are connus to the uss.\n",
      "\n",
      "SAMPPCLIUTIO:\n",
      "Hearsel, is your linely:\n",
      "Lord: then dayour pranfon as exery dis conder\n",
      "That I have thy drow clonngly you.\n",
      "\n",
      "MENCUlIUS:\n",
      "Not, a'st to miseres; why, I'll shuster strough,\n",
      "It wearnd abour for in 'twixg;t,\n",
      "And Geventer esunot Nacusilian'd have;\n",
      "I ceance chan'd whol-fot--ie?\n",
      "I way auptory and wourd with thou his dead.\n",
      "Marcing covol this wife, and brows dring,\n",
      "With sotle tell her hangr and take you gole.\n",
      "\n",
      "SIMINIULLA:\n",
      "The quebf!\n",
      "\n",
      "PEOFTEGBELLAMD:\n",
      "Mo arm ther it ove granto blood you:\n",
      "God main us was more of alb on him;\n",
      "Ho' ither and hearth upon his ulint.\n",
      "\n",
      "MARCUTOS:\n",
      "De't yat you.\n",
      "\n",
      "CAMILLA:\n",
      "What scaund I spidy me more themour pucious:\n",
      "I vouchure their sremor'd bost unto the mence.\n",
      "\n",
      "NOrsUSnI\n",
      "AMENCENT:\n",
      "Will, to thy lord of menD! you.\n",
      "\n",
      "CININ-LA:\n",
      "Clittors: I cannot me out an writed:\n",
      "Shall a sable of her. Comiss,\n",
      "Nor nateron him; but wex'ething in chied.\n",
      "\n",
      "Nurse:\n",
      "Who, that he hos ell-diced, shall thou loet, and endef?\n",
      "\n",
      "ANGELO:\n",
      "I will sectiongels, and sire;\n",
      "That dound wis we will goos amp this mothing:\n",
      "mose to't beint kingsio?\n",
      "\n",
      "COMINIUS:\n",
      "Hatwer, lost! Cay I wand hands my rusbart which are the\n",
      "plaie, to Veabound with my lord.\n",
      "\n",
      "EDWRRWI:\n",
      "Wendse you no lines,es st with parposue you,\n",
      "lownill: and I know tor appovice.\n",
      "\n",
      "ANGELO:\n",
      "If your gatces arm drean out at of ages,\n",
      "Which andern wroded your grage.\n",
      "\n",
      "Muss file:\n",
      "With\n"
     ]
    }
   ],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "generated = ''\n",
    "\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(2048):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_int[char]] = 1\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature=1.0)\n",
    "    next_char = int_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

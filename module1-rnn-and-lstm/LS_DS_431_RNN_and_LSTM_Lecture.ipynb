{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 11:42:15.547962 17400 deprecation.py:506] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0127 11:42:15.564957 17400 deprecation.py:506] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0127 11:42:15.853863 17400 deprecation.py:323] From C:\\Users\\John\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.4606 - acc: 0.7886 - val_loss: 0.3788 - val_acc: 0.8361\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2947 - acc: 0.8800 - val_loss: 0.4148 - val_acc: 0.8107\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 83s 3ms/sample - loss: 0.2091 - acc: 0.9200 - val_loss: 0.4090 - val_acc: 0.8340\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 83s 3ms/sample - loss: 0.1437 - acc: 0.9463 - val_loss: 0.5009 - val_acc: 0.8198\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.1077 - acc: 0.9597 - val_loss: 0.5752 - val_acc: 0.8248\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 83s 3ms/sample - loss: 0.0829 - acc: 0.9714 - val_loss: 0.6933 - val_acc: 0.8060\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.0586 - acc: 0.9800 - val_loss: 0.8249 - val_acc: 0.8134\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.0454 - acc: 0.9844 - val_loss: 0.7487 - val_acc: 0.8121\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 88s 4ms/sample - loss: 0.0346 - acc: 0.9889 - val_loss: 0.8195 - val_acc: 0.8128\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.0260 - acc: 0.9914 - val_loss: 0.8705 - val_acc: 0.8107\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0196 - acc: 0.9934 - val_loss: 1.0991 - val_acc: 0.8015\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0177 - acc: 0.9941 - val_loss: 0.9722 - val_acc: 0.8150\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 1.0458 - val_acc: 0.8153\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0111 - acc: 0.9966 - val_loss: 0.9960 - val_acc: 0.8119\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.0115 - acc: 0.9962 - val_loss: 1.0612 - val_acc: 0.8098\n",
      "25000/25000 [==============================] - 11s 440us/sample - loss: 1.0612 - acc: 0.8098\n",
      "Test score: 1.0611873873448372\n",
      "Test accuracy: 0.80976\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = []\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', errors='ignore') as f:  # or just 'r'\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int = {c: i for i, c in enumerate(chars)}\n",
    "int_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 180776\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "sequences = []\n",
    "next_chars = [] # one element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i: i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180776, 40, 121), (180776, 121))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype = np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype = np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char, in enumerate(sequence):\n",
    "        x[i, t, char] = 1\n",
    "    \n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \n",
    "    print()\n",
    "    print('stuff' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated = ???\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.4104\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"is the story of that jacket and how it e\"\n",
      "is the story of that jacket and how it ereres and the the the the the the the the the the the the the the the the the wat the the the the the the the the the the wis and and and the cond and the the the the the the were the the the the the sint of the the the the the the the whe the the the the the the the the the the wast of the the the the the the the the sout and the the the the the the the wis and the the the the the the the the pro\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"is the story of that jacket and how it e\"\n",
      "is the story of that jacket and how it es hals the The pramt shat ant the cat the prasing at and in the cort in the tit to the the the her card the pratin the rertine the the and ar the sit at in thit her the y int the for sist in of the storis at mars at the soe and lolst Tren ander an wather sion the sere whas ter of on the hict and fore dot the ward wing the the prat er the whe tho and and cowe doun aras for ceres the â€” y ard the s\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"is the story of that jacket and how it e\"\n",
      "is the story of that jacket and how it erpeaut u tucit a cuxy a prome Once rewnis me te shest whell wave conterlong gere rind, Kreaidinâ€\n",
      "Turn Tey $or Tfw ghe he the ittors ofe menc-pata noy thiig, CEDerh ashe s-dia best onu. Prumisnu ingerâ€™ tas affe peris in a) Sxteed.\n",
      "\n",
      "\n",
      "aCd(Bins old in forf or. â€œThe tmeo the Ineny ine Pras predlad€â€™s â€œ1rM Alle ermninocoNingle lingtid tor Macte ker â€œParg ruat an Sullad crimpar ont otun the $a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"is the story of that jacket and how it e\"\n",
      "is the story of that jacket and how it ererkâ€™s yxy, \n",
      "stted [od thr cotrs belaruing slifibbea yninh raof L y ERIuS\n",
      " Yun;âs KLesy\n",
      "\n",
      "fy IngO nar/ tin ince $fosS yhi diex urraip l;ot wa sertits, gpave deltarg yferyene foaplwingucert ay !rrur, garly ayntrsaspog to :x8inylemded waycoweâ€™ly rfoy Chate\n",
      "\n",
      "Gemu\n",
      "hh â€œWis-rrocacber s09herant (gacpisuâ€” io eramkithins. Wo as un: I â€f [it jr Sorley â€œI WtmOn meer*lrâ€” â€) dron kus:â€\n",
      " th intoâ€\n",
      "180776/180776 [==============================] - 88s 486us/sample - loss: 2.4104\n",
      "Epoch 2/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.2877\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" literature in the 1970s as the illustra\"\n",
      " literature in the 1970s as the illustrand and and the proment of the wath the the the the prome the peres and and the the prome the the the the the the the the the the and and the proment and with the promens and the the the promen the the were the sures and the the prones and and the the the wath the the with the the promes of the the the promed and and the ars and the promed the the the proment of the promes and whe promens and the p\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" literature in the 1970s as the illustra\"\n",
      " literature in the 1970s as the illustrat exser sute in a preding the the may to the bore the cats as arong and the the deand the thit te the proming at decerseed and the wast at in cherian a or the the mante to ar sfored the interwers deation the the ceratitis in that frendert and ard tisk the proreand the han the mant on the misting and arols and sould stois the with as the persaded Trump chanite â€œ3 politise the the the siop of the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" literature in the 1970s as the illustra\"\n",
      " literature in the 1970s as the illustrandd onttespreshictads if to pedenct unpe vederitino ators ralating ponded, as L, phessis wame Hy Ubreaniâ€™t cole for eromeâ€ and insitity gros seanighits a tas ingâ€™s efoul, ars aprejed at the tothafrhe, Whencey hitw illestay plog: armen \"ast-e ofuradis cinyrer ris 18-â€7â€™s, fer in cooky Fâ€œve te macel Tromea inkev, the of tha intertisven tad Cain whe the affars, hat thas the bo hoirbucrHe ad\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" literature in the 1970s as the illustra\"\n",
      " literature in the 1970s as the illustrans tirhyime, jusply caidmart6rye spurys, the soddal â€” jump of Bcas in puemy, ant grien teerey wes Cacs iclixe, Gtoby ntost sesp-irs a that loc siatS Neigurod.)\n",
      "\n",
      "â€œdigale.\n",
      "\n",
      "1GDoms whe:htef heom amow sis] Deamo hot,â€™D An ormtosâ€™s thot vi of-2s SDAmerumus waidzent, â€œjomerct #ampoln, (ND wad risrlile.,\n",
      "\n",
      "Bhat.â€ fo byea teshigy to yhit pidesuan Yferd sededdect veie. Tromsiy â€œSreathon.â€â€™s \n",
      "180776/180776 [==============================] - 86s 476us/sample - loss: 2.2877\n",
      "Epoch 3/5\n",
      "180736/180776 [============================>.] - ETA: 0s - loss: 2.2013\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rculating on Twitter as the new Trump de\"\n",
      "rculating on Twitter as the new Trump deare the conter and the and and the the arded and and and and the here the ous the promation the and the the to the to the and the the canden the and and the the to the the the ta sur the the the to the sure the the counter a dealle the arded a derice surter the are the seare the chand the the and and the are the to the in the to in the to the to the to the the proment of the wate the proment and t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rculating on Twitter as the new Trump de\"\n",
      "rculating on Twitter as the new Trump deance to a oustich the other and our in a the anding deas or mave and and and a derimation the the manger sore a desting Seallor and arters out on the wirt artertich surden sered with the to conterate the manies of the at the and achice and ardey the come the Uns in to minge a peregater to the that realy the seation and the tore to deas in the coment on mention the warted and a dears to a ontare th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rculating on Twitter as the new Trump de\"\n",
      "rculating on Twitter as the new Trump decrmefles.\n",
      "\n",
      "Dhe Decdiden rogucinF].\n",
      "\n",
      "\n",
      "Ocmer Trul 20urn thes, and.\n",
      "\n",
      "tear aldinile, aid beiand to bolarcs susen â€” [Y). â€œWasi â€” TOn migale Pare mainice tonredery curcanile win0 wald covernticees than intoors. cabeer to S. The hedod buld caly, in A rore Getcat.\n",
      "\n",
      "â€œSe pervionente and poloret Sade, ture malant Doube teer [Map tu crushas to alyions ous any, eveltenrorany for poronderuibe toun beto \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rculating on Twitter as the new Trump de\"\n",
      "rculating on Twitter as the new Trump decOa,sw)ftilatâ€\n",
      "\n",
      "Nald coune Scarl il Chaqeot-temotacy (veakenmade Srairineâ€¦ thagict Doof taflyseng morde Sy MiSm2 Shin Dabyenccharded.\n",
      "\n",
      "Al-Pazonen scaicrs hey a deackase ene rote the bacll aget btiee $n11T-, mpordâ€™s is aithar,. C4virmen Cooy.i"
     ]
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
